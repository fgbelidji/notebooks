{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6e5f2b",
   "metadata": {},
   "source": [
    "# Use TorchServe to deploy model on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbbda7",
   "metadata": {},
   "source": [
    "Inspired by https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee9cbff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=./keys/huggingface-ml-e974975230cc.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS ./keys/huggingface-ml-e974975230cc.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4929b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"huggingface-ml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d25c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5e61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\r\n",
      "You should consider upgrading via the '/Users/florentgbelidji/.pyenv/versions/3.9.10/envs/venv_hf_3.9.10/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install --upgrade google-cloud-aiplatform\n",
    "!pip -q install transfomers\n",
    "!pip -q install 'optimum[onnxruntime]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c60c5",
   "metadata": {},
   "source": [
    "### Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0825e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"sentence-transformers/msmarco-distilbert-base-tas-b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "pt_save_directory = \"./predictor/model/\"\n",
    "\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b665aa",
   "metadata": {},
   "source": [
    "### Apply optimum optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7fa645f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxruntime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ORTModelForFeatureExtraction, ORTOptimizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptimizationConfig\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/envs/venv_hf_3.9.10/lib/python3.9/site-packages/optimum/onnxruntime/__init__.py:51\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m ORT_DEFAULT_CHANNEL_FOR_OPERATORS \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatMul\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m     48\u001b[0m ORT_FULLY_CONNECTED_OPERATORS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatMul\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ORTConfig\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ORTModel\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_ort\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     54\u001b[0m     ORTModelForCausalLM,\n\u001b[1;32m     55\u001b[0m     ORTModelForFeatureExtraction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     ORTModelForTokenClassification,\n\u001b[1;32m     59\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/envs/venv_hf_3.9.10/lib/python3.9/site-packages/optimum/onnxruntime/configuration.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version, parse\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphOptimizationLevel\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m ort_version\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CalibraterBase, CalibrationMethod, QuantFormat, QuantizationMode, QuantType\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnxruntime'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "from optimum.pipelines import pipeline\n",
    "\n",
    "\n",
    "pt_save_directory_optimum = \"./predictor/optimum/\"\n",
    "\n",
    "save_path = Path(\"optimum_model\")\n",
    "save_path.mkdir(exist_ok=True)\n",
    "\n",
    "#use ORTOptimizer to export the model and define quantization configuration\n",
    "optimizer = ORTOptimizer(model=model, tokenizer=tokenizer)\n",
    "optimization_config = OptimizationConfig(optimization_level=2)\n",
    "\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.export(\n",
    "    onnx_model_path=save_path / \"model.onnx\",\n",
    "    onnx_optimized_model_output_path=save_path / \"model-optimized.onnx\",\n",
    "    optimization_config=optimization_config,\n",
    ")\n",
    "\n",
    "optimizer.model.config.save_pretrained(save_path) # saves config.json \n",
    "\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(save_path, file_name=\"model-optimized.onnx\")\n",
    "\n",
    "tokenizer.save_pretrained(pt_save_directory_optimum)\n",
    "model.save_pretrained(pt_save_directory_optimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1b219",
   "metadata": {},
   "source": [
    "### Create handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf0aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9de29252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predictor/custom_handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predictor/custom_handler.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "from optimum.pipelines import pipeline\n",
    "\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "class SentenceTransformersHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    The handler takes an input string and returns the classification text \n",
    "    based on the serialized transformers checkpoint.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SentenceTransformersHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\" Loads the model.pt file and initialized the model object.\n",
    "        Instantiates Tokenizer for preprocessor to use\n",
    "        Loads labels to name mapping file for post-processing inference response\n",
    "        \"\"\"\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        #self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.pt or pytorch_model.bin file\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = ORTModelForFeatureExtraction.from_pretrained(model_dir)\n",
    "        logger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n",
    "        \n",
    "        # Ensure to use the same tokenizer used during training\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, model_max_length=128)\n",
    "        self.pipeline = pipeline(\"feature-extraction\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        \"\"\" Preprocessing input request by tokenizing\n",
    "            Extend with your own preprocessing steps as needed\n",
    "        \"\"\"\n",
    "        text = data[0].get(\"data\")\n",
    "        if text is None:\n",
    "            text = data[0].get(\"body\")\n",
    "        sentences = text.decode('utf-8')\n",
    "        logger.info(\"Received text: '%s'\", sentences)\n",
    "\n",
    "        # Tokenize the texts\n",
    "      #  tokenizer_args = ((sentences,))\n",
    "      #  inputs = self.tokenizer(*tokenizer_args,\n",
    "      #                          padding='max_length',\n",
    "      #                          max_length=128,\n",
    "      #                          truncation=True,\n",
    "      #                          return_tensors = \"pt\")\n",
    "        return sentences\n",
    "\n",
    "    def inference(self, sentences):\n",
    "        \"\"\" Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        \n",
    "        def cls_pooling(pipeline_output):\n",
    "            return [_h[0] for _h in pipeline_output]\n",
    "        \n",
    "        embeddings = cls_pooling(self.pipeline(sentences))\n",
    "\n",
    "        logger.info(f\"Model embedded: {len(embeddings)}\" )\n",
    "        return embeddings\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        return inference_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44f516",
   "metadata": {},
   "source": [
    "### Write Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0382f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"test_sbert_embedder_optimum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca79194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./predictor/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat << EOF > ./predictor/Dockerfile\n",
    "\n",
    "FROM pytorch/torchserve:latest-cpu\n",
    "\n",
    "# install dependencies\n",
    "RUN python3 -m pip install --upgrade pip\n",
    "RUN pip3 install transformers\n",
    "RUN pip3 install 'optimum[onnxruntime]'\n",
    "\n",
    "USER model-server\n",
    "\n",
    "# copy model artifacts, custom handler and other dependencies\n",
    "COPY custom_handler.py /home/model-server/\n",
    "COPY ./optimum/ / /home/model-server/\n",
    "\n",
    "# create torchserve configuration file\n",
    "USER root\n",
    "RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nworkers=4\" >> /home/model-server/config.properties\n",
    "\n",
    "# expose health and prediction listener ports from the image\n",
    "EXPOSE 7080\n",
    "EXPOSE 7081\n",
    "\n",
    "# create model archive file packaging model artifacts and dependencies\n",
    "RUN torch-model-archiver -f \\\n",
    "  --model-name=$APP_NAME \\\n",
    "  --version=1.0 \\\n",
    "  --serialized-file=/home/model-server/model.onnx \\\n",
    "  --handler=/home/model-server/custom_handler.py \\\n",
    "  --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt\" \\\n",
    "  --export-path=/home/model-server/model-store\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "CMD [\"torchserve\", \\\n",
    "     \"--start\", \\\n",
    "     \"--ts-config=/home/model-server/config.properties\", \\\n",
    "     \"--models\", \\\n",
    "     \"$APP_NAME=$APP_NAME.mar\", \\\n",
    "     \"--model-store\", \\\n",
    "     \"/home/model-server/model-store\"]\n",
    "EOF\n",
    "\n",
    "echo \"Writing ./predictor/Dockerfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e2b160e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM_PREDICTOR_IMAGE_URI = gcr.io/huggingface-ml/pytorch_predict_test_sbert_embedder_optimum\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
    "print(f\"CUSTOM_PREDICTOR_IMAGE_URI = {CUSTOM_PREDICTOR_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca594d5b",
   "metadata": {},
   "source": [
    "### Build container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7d09ce47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.1s\n",
      " => => transferring context: 2.34MB                                        0.1s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.9s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.2s\n",
      " => => transferring context: 4.96MB                                        0.2s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.1s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.4s\n",
      " => => transferring context: 8.10MB                                        0.4s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.2s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.5s\n",
      " => => transferring context: 11.71MB                                       0.5s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.3s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.6s\n",
      " => => transferring context: 18.33MB                                       0.6s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.4s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.7s\n",
      " => => transferring context: 22.03MB                                       0.7s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        0.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.5s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.8s\n",
      " => => transferring context: 25.54MB                                       0.8s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.7s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          1.0s\n",
      " => => transferring context: 28.98MB                                       1.0s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        1.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.8s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          1.1s\n",
      " => => transferring context: 31.97MB                                       1.1s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.9s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          1.2s\n",
      " => => transferring context: 39.37MB                                       1.2s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.0s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          1.3s\n",
      " => => transferring context: 42.95MB                                       1.3s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        1.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.1s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          1.4s\n",
      " => => transferring context: 46.58MB                                       1.4s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        1.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.2s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          1.5s\n",
      " => => transferring context: 50.35MB                                       1.5s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        1.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.4s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          1.7s\n",
      " => => transferring context: 54.16MB                                       1.7s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        1.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.5s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          1.8s\n",
      " => => transferring context: 61.60MB                                       1.8s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        1.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.6s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          1.9s\n",
      " => => transferring context: 65.10MB                                       1.9s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        1.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.7s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          2.0s\n",
      " => => transferring context: 69.26MB                                       2.0s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        2.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.8s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          2.1s\n",
      " => => transferring context: 73.36MB                                       2.1s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        2.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.9s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          2.2s\n",
      " => => transferring context: 76.97MB                                       2.2s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        2.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.0s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          2.3s\n",
      " => => transferring context: 80.15MB                                       2.3s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        2.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.1s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          2.4s\n",
      " => => transferring context: 83.56MB                                       2.4s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        2.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.2s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          2.5s\n",
      " => => transferring context: 87.39MB                                       2.5s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        2.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.3s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          2.6s\n",
      " => => transferring context: 90.80MB                                       2.6s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        2.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.4s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          2.7s\n",
      " => => transferring context: 94.24MB                                       2.7s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        2.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.6s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          2.9s\n",
      " => => transferring context: 97.71MB                                       2.9s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        2.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.7s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          3.0s\n",
      " => => transferring context: 104.83MB                                      3.0s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.9s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          3.2s\n",
      " => => transferring context: 108.04MB                                      3.2s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        3.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.0s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          3.3s\n",
      " => => transferring context: 111.35MB                                      3.3s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        3.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.1s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          3.4s\n",
      " => => transferring context: 118.26MB                                      3.4s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        3.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.3s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          3.6s\n",
      " => => transferring context: 122.17MB                                      3.6s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        3.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.4s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          3.7s\n",
      " => => transferring context: 125.67MB                                      3.7s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        3.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.5s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          3.8s\n",
      " => => transferring context: 132.78MB                                      3.8s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        3.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.6s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          3.9s\n",
      " => => transferring context: 136.59MB                                      3.9s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        3.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.7s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          4.0s\n",
      " => => transferring context: 139.96MB                                      4.0s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        4.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.8s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          4.1s\n",
      " => => transferring context: 143.90MB                                      4.1s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        4.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.9s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          4.2s\n",
      " => => transferring context: 147.83MB                                      4.2s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        4.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.0s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          4.3s\n",
      " => => transferring context: 152.19MB                                      4.3s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        4.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.1s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          4.4s\n",
      " => => transferring context: 156.25MB                                      4.4s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        4.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.2s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          4.5s\n",
      " => => transferring context: 160.35MB                                      4.5s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        4.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.3s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          4.6s\n",
      " => => transferring context: 163.99MB                                      4.6s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        4.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.4s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          4.7s\n",
      " => => transferring context: 167.49MB                                      4.7s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        4.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.5s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          4.8s\n",
      " => => transferring context: 171.20MB                                      4.8s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        4.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.7s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          5.0s\n",
      " => => transferring context: 175.00MB                                      5.0s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        5.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.8s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          5.1s\n",
      " => => transferring context: 179.20MB                                      5.1s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        5.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.9s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          5.2s\n",
      " => => transferring context: 186.14MB                                      5.2s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        5.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.1s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          5.4s\n",
      " => => transferring context: 190.40MB                                      5.4s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        5.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.2s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          5.5s\n",
      " => => transferring context: 194.44MB                                      5.5s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        5.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.3s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          5.7s\n",
      " => => transferring context: 201.91MB                                      5.6s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        5.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.4s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          5.8s\n",
      " => => transferring context: 205.48MB                                      5.7s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        5.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.5s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          5.9s\n",
      " => => transferring context: 208.89MB                                      5.8s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        5.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.7s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          6.0s\n",
      " => => transferring context: 212.53MB                                      6.0s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        6.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.8s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          6.2s\n",
      " => => transferring context: 216.59MB                                      6.2s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        6.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.9s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          6.3s\n",
      " => => transferring context: 224.52MB                                      6.3s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        6.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.0s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          6.4s\n",
      " => => transferring context: 228.03MB                                      6.4s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        6.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.1s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          6.5s\n",
      " => => transferring context: 231.28MB                                      6.5s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        6.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.3s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          6.6s\n",
      " => => transferring context: 235.18MB                                      6.6s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        6.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.4s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          6.7s\n",
      " => => transferring context: 238.49MB                                      6.7s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        6.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.6s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          6.9s\n",
      " => => transferring context: 242.13MB                                      6.9s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        6.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.7s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          7.0s\n",
      " => => transferring context: 249.76MB                                      7.0s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        7.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.8s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          7.1s\n",
      " => => transferring context: 253.50MB                                      7.1s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        7.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.9s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          7.2s\n",
      " => => transferring context: 257.04MB                                      7.2s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        7.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.0s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          7.3s\n",
      " => => transferring context: 261.00MB                                      7.3s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        7.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.1s (6/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m => [internal] load build context                                          7.4s\n",
      " => => transferring context: 264.58MB                                      7.4s\n",
      "\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        7.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.2s (7/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        7.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.3s (7/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        7.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.5s (7/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        7.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.6s (7/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        7.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.8s (7/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        8.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.9s (7/16)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        8.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        8.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        8.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        8.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        8.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        9.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        9.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        9.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        9.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        9.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        9.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                        9.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       10.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       10.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       10.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       10.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       10.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       10.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       10.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       11.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       11.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       11.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       11.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       11.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       11.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       12.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       12.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       12.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       12.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       12.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       12.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       12.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       13.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       13.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       13.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       13.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       13.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       13.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       14.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       14.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       14.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       14.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       14.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       14.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       14.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       15.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       15.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       15.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       15.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       15.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       15.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       15.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       16.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       16.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       16.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       16.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       16.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       16.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       17.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 18.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       17.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       17.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       17.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       17.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       17.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       17.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       18.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 19.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       18.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 20.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       18.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 20.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       18.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 20.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       18.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 20.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       18.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 20.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       18.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 20.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       19.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 20.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       19.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 21.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       19.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 21.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       19.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 21.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       19.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 21.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       19.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 21.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       20.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 21.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       20.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 22.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       20.3s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 22.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       20.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 22.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       20.6s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 22.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       20.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 22.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       20.9s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 22.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       21.1s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 22.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       21.2s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 23.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       21.4s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 23.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       21.5s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 23.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       21.7s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 23.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       21.8s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 23.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       22.0s\n",
      "\u001b[2m => => # Collecting optimum[onnxruntime]                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 23.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       22.1s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 23.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       22.3s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 24.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       22.4s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 24.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       22.6s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 24.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       22.7s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 24.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       22.9s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 24.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       23.0s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 24.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       23.2s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 25.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       23.3s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 25.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       23.5s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 25.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       23.6s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 25.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       23.8s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 25.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       23.9s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 25.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       24.1s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 25.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       24.2s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 26.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       24.4s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 26.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       24.5s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 26.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       24.7s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 26.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       24.8s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 26.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       25.0s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 26.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       25.1s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 27.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       25.3s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 27.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       25.4s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 27.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       25.6s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 27.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       25.7s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 27.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       25.9s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 27.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       26.0s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 27.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       26.2s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 28.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       26.3s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 28.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       26.5s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 28.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       26.6s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 28.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       26.8s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 28.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       26.9s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 28.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       27.1s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 28.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       27.2s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 29.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       27.4s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 29.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       27.5s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 29.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       27.7s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 29.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       27.8s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 29.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       28.0s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 29.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       28.1s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 30.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       28.3s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 30.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       28.4s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 30.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       28.6s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 30.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       28.7s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 30.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       28.9s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 30.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       29.0s\n",
      "\u001b[2m => => #   Downloading optimum-1.2.3.tar.gz (75 kB)                            \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 1.0 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 30.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       29.2s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 31.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       29.3s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 31.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       29.5s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 31.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       29.6s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 31.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       29.8s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 31.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       29.9s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 31.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       30.1s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 31.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       30.3s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 32.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       30.4s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 32.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       30.6s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 32.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       30.7s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 32.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       30.9s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 32.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       31.0s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 32.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       31.2s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 33.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       31.3s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 33.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       31.5s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 33.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       31.6s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 33.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       31.8s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 33.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       31.9s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 33.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       32.1s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 33.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       32.2s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 34.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       32.4s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 34.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       32.5s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 34.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       32.7s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 34.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       32.8s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 34.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       33.0s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 34.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       33.1s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 35.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       33.3s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 35.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       33.4s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 35.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       33.6s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 35.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       33.7s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 35.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       33.9s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 35.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       34.0s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 35.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       34.2s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 36.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       34.3s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 36.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       34.5s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 36.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       34.6s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 36.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       34.8s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 36.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       34.9s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 36.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       35.1s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 36.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       35.2s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 37.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       35.4s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 37.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       35.5s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 37.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       35.7s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 37.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       35.8s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 37.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       36.0s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 37.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       36.1s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 38.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       36.3s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 38.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       36.4s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 38.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       36.6s\n",
      "\u001b[2m => => #   Installing build dependencies: started                              \n",
      "\u001b[0m\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 38.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       36.7s\n",
      "\u001b[2m => => #   Installing build dependencies: finished with status 'done'          \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 38.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       36.9s\n",
      "\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 38.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       37.0s\n",
      "\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 38.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       37.2s\n",
      "\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 39.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       37.3s\n",
      "\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 39.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       37.5s\n",
      "\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 39.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       37.6s\n",
      "\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 39.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       37.8s\n",
      "\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 39.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       37.9s\n",
      "\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 39.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       38.1s\n",
      "\u001b[2m => => #   Getting requirements to build wheel: started                        \n",
      "\u001b[0m\u001b[2m => => #   Getting requirements to build wheel: finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 39.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       38.2s\n",
      "\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/6.4 MB 5.3 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 40.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       38.4s\n",
      "\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/6.4 MB 5.3 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 40.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       38.5s\n",
      "\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/6.4 MB 5.3 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 40.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       38.7s\n",
      "\u001b[2m => => #   Preparing metadata (pyproject.toml): started                        \n",
      "\u001b[0m\u001b[2m => => #   Preparing metadata (pyproject.toml): finished with status 'done'    \n",
      "\u001b[0m\u001b[2m => => # Collecting sympy                                                      \n",
      "\u001b[0m\u001b[2m => => #   Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)                  \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/6.4 MB 5.3 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 40.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       38.8s\n",
      "\u001b[2m => => # Collecting coloredlogs                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)         \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 6.6 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: torch>=1.9 in /home/venv/lib/python3.8/\n",
      "\u001b[0m\u001b[2m => => # site-packages (from optimum[onnxruntime]) (1.11.0+cpu)                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 40.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       38.9s\n",
      "\u001b[2m => => # /home/venv/lib/python3.8/site-packages (from optimum[onnxruntime]) (4.\n",
      "\u001b[0m\u001b[2m => => # 20.0)                                                                 \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: huggingface-hub>=0.4.0 in /home/venv/li\n",
      "\u001b[0m\u001b[2m => => # b/python3.8/site-packages (from optimum[onnxruntime]) (0.7.0)         \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 40.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       39.1s\n",
      "\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 40.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       39.2s\n",
      "\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 41.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       39.4s\n",
      "\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 41.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       39.5s\n",
      "\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 41.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       39.7s\n",
      "\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 41.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       39.8s\n",
      "\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 41.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       40.0s\n",
      "\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 41.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       40.1s\n",
      "\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 42.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       40.3s\n",
      "\u001b[2m => => # Requirement already satisfied: packaging in /home/venv/lib/python3.8/s\n",
      "\u001b[0m\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 42.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       40.4s\n",
      "\u001b[2m => => # ite-packages (from optimum[onnxruntime]) (21.3)                       \n",
      "\u001b[0m\u001b[2m => => # Collecting datasets>=1.2.1                                            \n",
      "\u001b[0m\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting protobuf==3.20.1                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 42.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       40.5s\n",
      "\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting protobuf==3.20.1                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux\n",
      "\u001b[0m\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 42.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       40.7s\n",
      "\u001b[2m => => #   Downloading datasets-2.3.2-py3-none-any.whl (362 kB)                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 8.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting protobuf==3.20.1                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux\n",
      "\u001b[0m\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 42.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       40.8s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting protobuf==3.20.1                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux\n",
      "\u001b[0m\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 42.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       41.0s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting protobuf==3.20.1                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux\n",
      "\u001b[0m\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 42.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       41.1s\n",
      "\u001b[2m => => # Collecting protobuf==3.20.1                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux\n",
      "\u001b[0m\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 42.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       41.2s\n",
      "\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 43.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       41.4s\n",
      "\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 43.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       41.6s\n",
      "\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 43.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       41.7s\n",
      "\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 43.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       41.9s\n",
      "\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 43.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       42.0s\n",
      "\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 43.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       42.2s\n",
      "\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 44.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       42.3s\n",
      "\u001b[2m => => # 1_x86_64.whl (1.0 MB)                                                 \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.6 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 44.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       42.4s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 44.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       42.6s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 44.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       42.7s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 44.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       42.9s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 44.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       43.0s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 44.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       43.2s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 45.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       43.3s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 45.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       43.5s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 45.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       43.6s\n",
      "\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 45.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       43.8s\n",
      "\u001b[2m => => # Collecting onnx                                                       \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux201\n",
      "\u001b[0m\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 45.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       44.0s\n",
      "\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 45.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       44.1s\n",
      "\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 45.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       44.3s\n",
      "\u001b[2m => => # 4_x86_64.whl (13.1 MB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 9.9 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 46.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       44.4s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 46.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       44.6s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 46.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       44.7s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 46.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       44.9s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 46.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       45.0s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 46.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       45.2s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 47.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       45.3s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 47.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       45.5s\n",
      "\u001b[2m => => # Collecting onnxruntime>=1.9.0                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[2m => => # Collecting responses<0.19                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 47.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       45.6s\n",
      "\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[2m => => # Collecting responses<0.19                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading responses-0.18.0-py3-none-any.whl (38 kB)               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 47.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       45.8s\n",
      "\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[2m => => # Collecting responses<0.19                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading responses-0.18.0-py3-none-any.whl (38 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 47.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       45.9s\n",
      "\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[2m => => # Collecting responses<0.19                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading responses-0.18.0-py3-none-any.whl (38 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 47.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       46.1s\n",
      "\u001b[2m => => #   Downloading onnxruntime-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manyl\n",
      "\u001b[0m\u001b[2m => => # inux2014_x86_64.whl (5.2 MB)                                          \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 12.6 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[2m => => # Collecting responses<0.19                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading responses-0.18.0-py3-none-any.whl (38 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 47.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       46.2s\n",
      "\u001b[2m => => #   Downloading responses-0.18.0-py3-none-any.whl (38 kB)               \n",
      "\u001b[0m\u001b[2m => => # Collecting xxhash                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 48.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       46.3s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 48.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       46.5s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 48.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       46.6s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 48.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       46.8s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 48.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       46.9s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 48.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       47.1s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 48.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       47.2s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 49.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       47.4s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 49.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       47.5s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 49.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       47.7s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 49.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       47.8s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 49.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       48.0s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 49.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       48.1s\n",
      "\u001b[2m => => # 14_x86_64.whl (212 kB)                                                \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 12.4 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 50.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       48.3s\n",
      "\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting aiohttp                                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_\n",
      "\u001b[0m\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 50.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       48.4s\n",
      "\u001b[2m => => # Requirement already satisfied: requests>=2.19.0 in /home/venv/lib/pyth\n",
      "\u001b[0m\u001b[2m => => # on3.8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.28\n",
      "\u001b[0m\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting aiohttp                                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_\n",
      "\u001b[0m\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 50.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       48.6s\n",
      "\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting aiohttp                                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_\n",
      "\u001b[0m\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 50.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       48.7s\n",
      "\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting aiohttp                                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_\n",
      "\u001b[0m\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 50.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       48.9s\n",
      "\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting aiohttp                                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_\n",
      "\u001b[0m\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 50.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       49.0s\n",
      "\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting aiohttp                                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_\n",
      "\u001b[0m\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 50.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       49.2s\n",
      "\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting aiohttp                                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_\n",
      "\u001b[0m\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 51.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       49.3s\n",
      "\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting aiohttp                                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_\n",
      "\u001b[0m\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 51.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       49.5s\n",
      "\u001b[2m => => # .0)                                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting aiohttp                                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_\n",
      "\u001b[0m\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 51.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       49.6s\n",
      "\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 51.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       49.8s\n",
      "\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 51.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       49.9s\n",
      "\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 51.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       50.1s\n",
      "\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 51.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       50.2s\n",
      "\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 52.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       50.4s\n",
      "\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 52.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       50.5s\n",
      "\u001b[2m => => # x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 7.5 MB/s eta \n",
      "\u001b[0m\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 52.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       50.7s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 10.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 52.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       50.8s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 10.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 52.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       51.0s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 10.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 52.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       51.1s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 10.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 53.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       51.3s\n",
      "\u001b[2m => => # 0:00:00                                                               \n",
      "\u001b[0m\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 10.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 53.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       51.4s\n",
      "\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 10.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting multiprocess                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 53.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       51.5s\n",
      "\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 10.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting multiprocess                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 53.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       51.7s\n",
      "\u001b[2m => => # Collecting pandas                                                     \n",
      "\u001b[0m\u001b[2m => => #   Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux20\n",
      "\u001b[0m\u001b[2m => => # 14_x86_64.whl (11.7 MB)                                               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 10.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting multiprocess                                               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 53.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       51.8s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 10.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting multiprocess                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)         \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.4/131.4 kB 6.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 53.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       51.9s\n",
      "\u001b[2m => => # Collecting multiprocess                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)         \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.4/131.4 kB 6.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: tqdm>=4.62.1 in /home/venv/lib/python3.\n",
      "\u001b[0m\u001b[2m => => # 8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.64.0) \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 53.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       52.1s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.4/131.4 kB 6.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: tqdm>=4.62.1 in /home/venv/lib/python3.\n",
      "\u001b[0m\u001b[2m => => # 8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.64.0) \n",
      "\u001b[0m\u001b[2m => => # Collecting fsspec[http]>=2021.05.0                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 53.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       52.3s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.4/131.4 kB 6.1 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: tqdm>=4.62.1 in /home/venv/lib/python3.\n",
      "\u001b[0m\u001b[2m => => # 8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.64.0) \n",
      "\u001b[0m\u001b[2m => => # Collecting fsspec[http]>=2021.05.0                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 54.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       52.4s\n",
      "\u001b[2m => => # Requirement already satisfied: tqdm>=4.62.1 in /home/venv/lib/python3.\n",
      "\u001b[0m\u001b[2m => => # 8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.64.0) \n",
      "\u001b[0m\u001b[2m => => # Collecting fsspec[http]>=2021.05.0                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 54.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       52.6s\n",
      "\u001b[2m => => # Requirement already satisfied: tqdm>=4.62.1 in /home/venv/lib/python3.\n",
      "\u001b[0m\u001b[2m => => # 8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.64.0) \n",
      "\u001b[0m\u001b[2m => => # Collecting fsspec[http]>=2021.05.0                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 54.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       52.7s\n",
      "\u001b[2m => => # Requirement already satisfied: tqdm>=4.62.1 in /home/venv/lib/python3.\n",
      "\u001b[0m\u001b[2m => => # 8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.64.0) \n",
      "\u001b[0m\u001b[2m => => # Collecting fsspec[http]>=2021.05.0                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 54.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       52.8s\n",
      "\u001b[2m => => # 8/site-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.64.0) \n",
      "\u001b[0m\u001b[2m => => # Collecting fsspec[http]>=2021.05.0                                    \n",
      "\u001b[0m\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 54.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       53.0s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 54.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       53.1s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 55.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       53.3s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 55.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       53.4s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 55.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       53.6s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 55.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       53.8s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 55.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       53.9s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 55.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       54.1s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 55.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       54.2s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 56.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       54.4s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 56.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       54.5s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 56.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       54.7s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 56.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       54.8s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 56.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       55.0s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 56.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       55.1s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 56.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       55.3s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 57.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       55.4s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 57.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       55.6s\n",
      "\u001b[2m => => #   Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 495.9 kB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 57.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       55.7s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 57.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       55.9s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 57.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       56.0s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 57.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       56.2s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 58.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       56.3s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 58.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       56.5s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 58.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       56.6s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 58.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       56.8s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 58.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       56.9s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 58.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       57.1s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 58.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       57.2s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 59.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       57.3s\n",
      "\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting dill<0.3.6                                                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 59.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       57.5s\n",
      "\u001b[2m => => # Collecting pyarrow>=6.0.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2\n",
      "\u001b[0m\u001b[2m => => # 014_x86_64.whl (29.4 MB)                                              \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting dill<0.3.6                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 59.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       57.6s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 8.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting dill<0.3.6                                                 \n",
      "\u001b[0m\u001b[2m => => #   Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 6.4 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 59.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       57.8s\n",
      "\u001b[2m => => # te-packages (from huggingface-hub>=0.4.0->optimum[onnxruntime]) (3.7.1\n",
      "\u001b[0m\u001b[2m => => # )                                                                     \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ven\n",
      "\u001b[0m\u001b[2m => => # v/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->optimum[on\n",
      "\u001b[0m\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 59.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       57.9s\n",
      "\u001b[2m => => # te-packages (from huggingface-hub>=0.4.0->optimum[onnxruntime]) (3.7.1\n",
      "\u001b[0m\u001b[2m => => # )                                                                     \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ven\n",
      "\u001b[0m\u001b[2m => => # v/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->optimum[on\n",
      "\u001b[0m\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 59.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       58.1s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 59.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       58.2s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 60.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       58.4s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 60.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       58.5s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 60.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       58.7s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 60.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       58.8s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 60.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       59.0s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 60.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       59.1s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 61.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       59.3s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 61.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       59.4s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 61.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       59.6s\n",
      "\u001b[2m => => # nxruntime]) (4.2.0)                                                   \n",
      "\u001b[0m\u001b[2m => => # Collecting flatbuffers                                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/venv/\n",
      "\u001b[0m\u001b[2m => => # lib/python3.8/site-packages (from packaging->optimum[onnxruntime]) (3.\n",
      "\u001b[0m\u001b[2m => => # 0.9)                                                                  \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 61.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       59.7s\n",
      "\u001b[2m => => # Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /h\n",
      "\u001b[0m\u001b[2m => => # ome/venv/lib/python3.8/site-packages (from transformers[sentencepiece]\n",
      "\u001b[0m\u001b[2m => => # >=4.15.0->optimum[onnxruntime]) (0.12.1)                              \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: regex!=2019.12.17 in /home/venv/lib/pyt\n",
      "\u001b[0m\u001b[2m => => # hon3.8/site-packages (from transformers[sentencepiece]>=4.15.0->optimu\n",
      "\u001b[0m\u001b[2m => => # m[onnxruntime]) (2022.6.2)                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 61.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       59.9s\n",
      "\u001b[2m => => # Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /h\n",
      "\u001b[0m\u001b[2m => => # ome/venv/lib/python3.8/site-packages (from transformers[sentencepiece]\n",
      "\u001b[0m\u001b[2m => => # >=4.15.0->optimum[onnxruntime]) (0.12.1)                              \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: regex!=2019.12.17 in /home/venv/lib/pyt\n",
      "\u001b[0m\u001b[2m => => # hon3.8/site-packages (from transformers[sentencepiece]>=4.15.0->optimu\n",
      "\u001b[0m\u001b[2m => => # m[onnxruntime]) (2022.6.2)                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 61.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       60.0s\n",
      "\u001b[2m => => # Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /h\n",
      "\u001b[0m\u001b[2m => => # ome/venv/lib/python3.8/site-packages (from transformers[sentencepiece]\n",
      "\u001b[0m\u001b[2m => => # >=4.15.0->optimum[onnxruntime]) (0.12.1)                              \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: regex!=2019.12.17 in /home/venv/lib/pyt\n",
      "\u001b[0m\u001b[2m => => # hon3.8/site-packages (from transformers[sentencepiece]>=4.15.0->optimu\n",
      "\u001b[0m\u001b[2m => => # m[onnxruntime]) (2022.6.2)                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 61.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       60.2s\n",
      "\u001b[2m => => # Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /h\n",
      "\u001b[0m\u001b[2m => => # ome/venv/lib/python3.8/site-packages (from transformers[sentencepiece]\n",
      "\u001b[0m\u001b[2m => => # >=4.15.0->optimum[onnxruntime]) (0.12.1)                              \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: regex!=2019.12.17 in /home/venv/lib/pyt\n",
      "\u001b[0m\u001b[2m => => # hon3.8/site-packages (from transformers[sentencepiece]>=4.15.0->optimu\n",
      "\u001b[0m\u001b[2m => => # m[onnxruntime]) (2022.6.2)                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 62.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       60.3s\n",
      "\u001b[2m => => # Requirement already satisfied: regex!=2019.12.17 in /home/venv/lib/pyt\n",
      "\u001b[0m\u001b[2m => => # hon3.8/site-packages (from transformers[sentencepiece]>=4.15.0->optimu\n",
      "\u001b[0m\u001b[2m => => # m[onnxruntime]) (2022.6.2)                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting sentencepiece!=0.1.92,>=0.1.91                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.man\n",
      "\u001b[0m\u001b[2m => => # ylinux2014_x86_64.whl (1.2 MB)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 62.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       60.5s\n",
      "\u001b[2m => => # m[onnxruntime]) (2022.6.2)                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting sentencepiece!=0.1.92,>=0.1.91                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.man\n",
      "\u001b[0m\u001b[2m => => # ylinux2014_x86_64.whl (1.2 MB)                                        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 16.1 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 62.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       60.6s\n",
      "\u001b[2m => => # m[onnxruntime]) (2022.6.2)                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting sentencepiece!=0.1.92,>=0.1.91                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.man\n",
      "\u001b[0m\u001b[2m => => # ylinux2014_x86_64.whl (1.2 MB)                                        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 16.1 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 62.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       60.8s\n",
      "\u001b[2m => => # m[onnxruntime]) (2022.6.2)                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting sentencepiece!=0.1.92,>=0.1.91                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.man\n",
      "\u001b[0m\u001b[2m => => # ylinux2014_x86_64.whl (1.2 MB)                                        \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 16.1 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 62.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       60.9s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 16.1 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[2m => => # Collecting humanfriendly>=9.1                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)         \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 10.3 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 62.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       61.0s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 16.1 MB/s eta\n",
      "\u001b[0m\u001b[2m => => #  0:00:00                                                              \n",
      "\u001b[0m\u001b[2m => => # Collecting humanfriendly>=9.1                                         \n",
      "\u001b[0m\u001b[2m => => #   Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)         \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 10.3 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 62.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       61.2s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 10.3 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting mpmath>=0.19                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)                  \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 532.6/532.6 kB 12.0 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 63.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       61.3s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 10.3 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting mpmath>=0.19                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)                  \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 532.6/532.6 kB 12.0 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 63.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       61.5s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 10.3 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting mpmath>=0.19                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)                  \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 532.6/532.6 kB 12.0 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 63.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       61.6s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 63.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       61.8s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 63.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       61.9s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 63.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       62.1s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 63.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       62.2s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 64.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       62.4s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 64.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       62.5s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 64.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       62.7s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 64.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       62.8s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 64.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       63.0s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 64.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       63.1s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 65.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       63.3s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 65.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       63.4s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 65.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       63.6s\n",
      "\u001b[2m => => # Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/py\n",
      "\u001b[0m\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 65.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       63.7s\n",
      "\u001b[2m => => # thon3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->optimum\n",
      "\u001b[0m\u001b[2m => => # [onnxruntime]) (2022.5.18.1)                                          \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: charset-normalizer~=2.0.0 in /home/venv\n",
      "\u001b[0m\u001b[2m => => # /lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.2.1->\n",
      "\u001b[0m\u001b[2m => => # optimum[onnxruntime]) (2.0.12)                                        \n",
      "\u001b[0m\u001b[2m => => # Collecting multidict<7.0,>=4.5                                        \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 65.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       63.9s\n",
      "\u001b[2m => => # Collecting multidict<7.0,>=4.5                                        \n",
      "\u001b[0m\u001b[2m => => #   Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x2014_x86_64.whl (121 kB)                                             \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 8.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting async-timeout<5.0,>=4.0.0a3                                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 65.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       64.0s\n",
      "\u001b[2m => => #   Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x2014_x86_64.whl (121 kB)                                             \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 8.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting async-timeout<5.0,>=4.0.0a3                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)           \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 65.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       64.2s\n",
      "\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 8.9 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting async-timeout<5.0,>=4.0.0a3                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)           \n",
      "\u001b[0m\u001b[2m => => # Collecting attrs>=17.3.0                                              \n",
      "\u001b[0m\u001b[2m => => #   Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 66.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       64.3s\n",
      "\u001b[2m => => # Collecting async-timeout<5.0,>=4.0.0a3                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)           \n",
      "\u001b[0m\u001b[2m => => # Collecting attrs>=17.3.0                                              \n",
      "\u001b[0m\u001b[2m => => #   Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.6/60.6 kB 5.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 66.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       64.5s\n",
      "\u001b[2m => => # Collecting async-timeout<5.0,>=4.0.0a3                                \n",
      "\u001b[0m\u001b[2m => => #   Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)           \n",
      "\u001b[0m\u001b[2m => => # Collecting attrs>=17.3.0                                              \n",
      "\u001b[0m\u001b[2m => => #   Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.6/60.6 kB 5.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 66.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       64.6s\n",
      "\u001b[2m => => #   Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.6/60.6 kB 5.8 MB/s et\n",
      "\u001b[0m\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting frozenlist>=1.1.1                                          \n",
      "\u001b[0m\u001b[2m => => #   Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 66.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       64.8s\n",
      "\u001b[2m => => # a 0:00:00                                                             \n",
      "\u001b[0m\u001b[2m => => # Collecting frozenlist>=1.1.1                                          \n",
      "\u001b[0m\u001b[2m => => #   Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 13.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 66.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       64.9s\n",
      "\u001b[2m => => #   Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 13.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting aiosignal>=1.1.2                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 66.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       65.0s\n",
      "\u001b[2m => => #   Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 13.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting aiosignal>=1.1.2                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 66.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       65.2s\n",
      "\u001b[2m => => #   Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 13.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting aiosignal>=1.1.2                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 67.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       65.3s\n",
      "\u001b[2m => => #   Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 13.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting aiosignal>=1.1.2                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 67.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       65.5s\n",
      "\u001b[2m => => #   Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 13.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting aiosignal>=1.1.2                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 67.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       65.6s\n",
      "\u001b[2m => => #   Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 13.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting aiosignal>=1.1.2                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 67.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       65.8s\n",
      "\u001b[2m => => #   Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinu\n",
      "\u001b[0m\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 13.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting aiosignal>=1.1.2                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 67.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       65.9s\n",
      "\u001b[2m => => # x1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)     \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 13.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Collecting aiosignal>=1.1.2                                           \n",
      "\u001b[0m\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n",
      "\u001b[0m\u001b[2m => => # Collecting yarl<2.0,>=1.0                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 67.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       66.0s\n",
      "\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n",
      "\u001b[0m\u001b[2m => => # Collecting yarl<2.0,>=1.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86\n",
      "\u001b[0m\u001b[2m => => # _64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)           \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 12.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 67.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       66.2s\n",
      "\u001b[2m => => #   Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)               \n",
      "\u001b[0m\u001b[2m => => # Collecting yarl<2.0,>=1.0                                             \n",
      "\u001b[0m\u001b[2m => => #   Downloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86\n",
      "\u001b[0m\u001b[2m => => # _64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)           \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 12.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 68.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       66.4s\n",
      "\u001b[2m => => # _64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)           \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 12.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: python-dateutil>=2.8.1 in /home/venv/li\n",
      "\u001b[0m\u001b[2m => => # b/python3.8/site-packages (from pandas->datasets>=1.2.1->optimum[onnxr\n",
      "\u001b[0m\u001b[2m => => # untime]) (2.8.2)                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 68.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       66.5s\n",
      "\u001b[2m => => # _64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)           \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 12.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: python-dateutil>=2.8.1 in /home/venv/li\n",
      "\u001b[0m\u001b[2m => => # b/python3.8/site-packages (from pandas->datasets>=1.2.1->optimum[onnxr\n",
      "\u001b[0m\u001b[2m => => # untime]) (2.8.2)                                                      \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 68.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       66.7s\n",
      "\u001b[2m => => # _64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)           \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 12.6 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: python-dateutil>=2.8.1 in /home/venv/li\n",
      "\u001b[0m\u001b[2m => => # b/python3.8/site-packages (from pandas->datasets>=1.2.1->optimum[onnxr\n",
      "\u001b[0m\u001b[2m => => # untime]) (2.8.2)                                                      \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 68.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       66.8s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: python-dateutil>=2.8.1 in /home/venv/li\n",
      "\u001b[0m\u001b[2m => => # b/python3.8/site-packages (from pandas->datasets>=1.2.1->optimum[onnxr\n",
      "\u001b[0m\u001b[2m => => # untime]) (2.8.2)                                                      \n",
      "\u001b[0m\u001b[2m => => # Collecting pytz>=2020.1                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)               \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 68.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       66.9s\n",
      "\u001b[2m => => # b/python3.8/site-packages (from pandas->datasets>=1.2.1->optimum[onnxr\n",
      "\u001b[0m\u001b[2m => => # untime]) (2.8.2)                                                      \n",
      "\u001b[0m\u001b[2m => => # Collecting pytz>=2020.1                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 503.5/503.5 kB 2.5 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 68.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       67.1s\n",
      "\u001b[2m => => # b/python3.8/site-packages (from pandas->datasets>=1.2.1->optimum[onnxr\n",
      "\u001b[0m\u001b[2m => => # untime]) (2.8.2)                                                      \n",
      "\u001b[0m\u001b[2m => => # Collecting pytz>=2020.1                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 503.5/503.5 kB 2.5 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 68.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       67.2s\n",
      "\u001b[2m => => # b/python3.8/site-packages (from pandas->datasets>=1.2.1->optimum[onnxr\n",
      "\u001b[0m\u001b[2m => => # untime]) (2.8.2)                                                      \n",
      "\u001b[0m\u001b[2m => => # Collecting pytz>=2020.1                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 503.5/503.5 kB 2.5 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 69.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       67.4s\n",
      "\u001b[2m => => # b/python3.8/site-packages (from pandas->datasets>=1.2.1->optimum[onnxr\n",
      "\u001b[0m\u001b[2m => => # untime]) (2.8.2)                                                      \n",
      "\u001b[0m\u001b[2m => => # Collecting pytz>=2020.1                                               \n",
      "\u001b[0m\u001b[2m => => #   Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 503.5/503.5 kB 2.5 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 69.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       67.5s\n",
      "\u001b[2m => => #   Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 503.5/503.5 kB 2.5 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 69.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       67.7s\n",
      "\u001b[2m => => #   Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)               \n",
      "\u001b[0m\u001b[2m => => #      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 503.5/503.5 kB 2.5 MB/s e\n",
      "\u001b[0m\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 69.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       67.8s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 69.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       68.0s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 69.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       68.1s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 70.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       68.3s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 70.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       68.4s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 70.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       68.6s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 70.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       68.7s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 70.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       68.9s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 70.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       69.0s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 70.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       69.2s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 71.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       69.3s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 71.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       69.5s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 71.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       69.6s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 71.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       69.8s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 71.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       69.9s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 71.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       70.1s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 71.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       70.2s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 72.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       70.4s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 72.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       70.5s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 72.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       70.7s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 72.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       70.8s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 72.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       71.0s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 72.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       71.1s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 73.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       71.3s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 73.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       71.4s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 73.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       71.6s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 73.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       71.7s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 73.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       71.9s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 73.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       72.1s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 73.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       72.2s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 74.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       72.4s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 74.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       72.5s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 74.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       72.7s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 74.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       72.8s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 74.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       73.0s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 74.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       73.1s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 75.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       73.3s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 75.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       73.4s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 75.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       73.6s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 75.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       73.7s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 75.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       73.9s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 75.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       74.0s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 75.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       74.2s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 76.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       74.3s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 76.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       74.5s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 76.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       74.6s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 76.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       74.8s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 76.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       74.9s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 76.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       75.1s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 76.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       75.2s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 77.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       75.4s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 77.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       75.5s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 77.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       75.7s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 77.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       75.8s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 77.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       76.0s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 77.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       76.1s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 78.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       76.3s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 78.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       76.4s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 78.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       76.6s\n",
      "\u001b[2m => => # ta 0:00:00                                                            \n",
      "\u001b[0m\u001b[2m => => # Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.8/si\n",
      "\u001b[0m\u001b[2m => => # te-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.2.1->opt\n",
      "\u001b[0m\u001b[2m => => # imum[onnxruntime]) (1.16.0)                                           \n",
      "\u001b[0m\u001b[2m => => # Building wheels for collected packages: optimum                       \n",
      "\u001b[0m\u001b[2m => => #   Building wheel for optimum (pyproject.toml): started                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 78.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       76.7s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 78.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       76.9s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 78.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       77.0s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 78.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       77.2s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 79.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       77.3s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 79.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       77.5s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 79.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       77.6s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 79.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       77.8s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 79.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       77.9s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 79.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       78.1s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 79.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       78.2s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 80.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       78.4s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 80.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       78.5s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 80.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       78.7s\n",
      "\u001b[2m => => #   Created wheel for optimum: filename=optimum-1.2.3-py3-none-any.whl s\n",
      "\u001b[0m\u001b[2m => => # ize=92434 sha256=0d04306995b12e6dee2536c2941107a22c1942bcc4c346557556b\n",
      "\u001b[0m\u001b[2m => => # d6baafa31ee                                                           \n",
      "\u001b[0m\u001b[2m => => #   Stored in directory: /home/model-server/.cache/pip/wheels/ae/93/ef/e\n",
      "\u001b[0m\u001b[2m => => # 86f1d9f1c81dffe193c2cb0f3bb70bb22bcedf374689f045d                     \n",
      "\u001b[0m\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 80.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       78.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 80.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       79.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 80.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       79.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 81.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       79.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 81.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       79.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 81.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       79.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 81.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       79.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 81.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       79.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 81.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       80.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 81.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       80.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 82.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       80.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 82.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       80.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 82.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       80.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 82.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       80.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 82.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       81.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 82.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       81.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 82.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       81.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 83.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       81.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 83.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       81.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 83.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       81.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 83.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       81.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 83.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       82.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 83.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       82.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 84.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       82.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 84.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       82.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 84.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       82.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 84.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       82.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 84.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       82.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 84.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       83.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 84.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       83.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 85.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       83.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 85.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       83.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 85.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       83.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 85.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       83.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 85.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       84.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 85.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       84.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 86.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       84.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 86.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       84.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 86.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       84.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 86.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       84.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 86.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       84.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 86.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       85.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 86.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       85.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 87.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       85.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 87.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       85.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 87.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       85.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 87.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       85.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 87.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       85.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 87.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       86.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 87.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       86.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 88.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       86.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 88.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       86.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 88.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       86.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 88.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       86.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 88.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       87.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 88.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       87.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 89.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       87.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 89.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       87.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 89.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       87.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 89.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       87.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 89.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       87.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 89.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       88.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 89.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       88.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 90.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       88.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 90.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       88.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 90.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       88.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 90.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       88.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 90.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       88.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 90.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       89.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 90.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       89.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 91.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       89.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 91.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       89.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 91.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       89.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 91.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       89.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 91.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       90.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 91.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       90.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 92.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       90.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 92.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       90.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 92.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       90.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 92.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       90.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 92.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       90.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 92.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       91.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 92.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       91.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 93.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       91.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 93.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       91.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 93.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       91.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 93.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       91.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 93.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       92.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 93.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       92.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 94.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       92.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 94.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       92.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 94.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       92.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 94.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       92.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 94.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       92.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 94.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       93.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 94.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       93.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 95.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       93.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 95.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       93.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 95.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       93.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 95.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       93.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 95.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       93.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 95.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       94.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 95.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       94.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 96.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       94.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 96.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       94.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 96.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       94.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 96.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       94.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 96.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       95.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 96.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       95.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 97.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       95.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 97.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       95.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 97.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       95.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 97.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       95.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 97.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       95.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 97.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       96.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 97.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       96.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 98.0s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       96.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 98.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       96.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 98.3s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       96.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 98.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       96.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 98.6s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       96.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 98.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       97.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 98.9s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       97.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 99.1s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       97.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 99.2s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       97.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 99.4s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       97.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 99.5s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       97.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 99.7s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       98.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 99.8s (7/16)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       98.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 100.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       98.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 100.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       98.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 100.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       98.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 100.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       98.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 100.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       98.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 100.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       99.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 100.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       99.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 101.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       99.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 101.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       99.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 101.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       99.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 101.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                       99.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 101.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      100.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 101.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      100.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 102.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      100.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 102.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      100.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 102.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      100.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 102.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      100.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 102.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      100.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 102.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      101.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 102.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      101.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 103.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      101.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 103.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      101.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 103.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      101.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 103.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      101.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 103.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      101.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 103.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      102.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 103.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      102.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 104.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      102.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 104.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      102.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 104.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      102.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 104.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      102.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 104.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      103.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 104.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      103.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 105.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      103.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 105.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      103.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 105.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      103.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 105.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      103.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 105.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      103.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 105.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      104.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 105.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      104.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 106.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      104.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 106.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      104.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 106.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      104.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 106.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      104.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 106.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      104.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 106.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      105.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 106.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      105.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 107.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      105.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 107.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      105.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 107.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      105.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 107.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      105.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 107.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      106.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 107.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      106.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 108.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      106.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 108.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      106.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 108.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      106.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 108.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      106.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 108.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      106.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 108.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      107.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 108.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      107.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 109.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      107.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 109.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      107.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 109.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      107.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 109.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      107.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 109.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      107.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 109.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      108.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 109.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      108.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 110.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      108.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 110.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      108.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 110.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      108.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 110.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      108.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 110.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      109.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 110.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      109.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 111.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      109.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 111.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      109.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 111.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      109.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 111.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      109.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 111.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      109.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 111.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      110.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 111.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      110.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 112.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      110.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 112.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      110.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 112.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      110.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 112.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      110.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 112.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      111.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 112.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      111.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 113.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      111.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 113.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      111.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 113.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      111.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 113.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      111.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 113.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      111.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 113.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      112.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 113.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      112.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 114.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      112.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 114.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      112.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 114.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      112.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 114.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      112.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 114.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      112.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 114.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      113.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 114.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      113.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 115.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      113.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 115.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      113.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 115.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      113.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 115.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      113.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 115.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      114.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 115.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      114.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 116.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      114.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 116.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      114.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 116.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      114.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 116.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      114.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 116.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      114.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 116.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      115.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 116.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      115.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 117.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      115.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 117.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      115.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 117.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      115.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 117.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      115.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 117.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      115.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 117.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      116.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 117.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      116.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 118.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      116.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 118.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      116.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 118.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      116.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 118.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      116.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 118.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      117.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 118.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      117.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 119.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      117.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 119.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      117.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 119.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      117.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 119.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      117.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 119.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      117.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 119.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      118.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 119.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      118.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 120.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      118.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 120.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      118.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 120.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      118.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 120.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      118.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 120.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      119.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 120.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      119.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 121.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      119.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 121.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      119.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 121.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      119.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 121.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      119.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 121.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      119.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 121.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      120.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 121.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      120.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 122.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      120.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 122.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      120.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 122.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      120.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 122.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      120.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 122.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      120.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 122.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      121.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 122.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      121.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 123.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      121.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 123.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      121.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 123.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      121.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 123.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      121.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 123.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      122.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 123.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      122.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 124.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      122.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 124.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      122.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 124.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      122.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 124.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      122.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 124.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      122.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 124.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      123.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 124.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      123.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 125.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      123.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 125.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      123.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 125.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      123.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 125.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      123.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 125.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      123.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 125.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      124.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 125.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      124.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 126.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      124.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 126.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      124.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 126.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      124.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 126.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      124.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 126.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      125.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 126.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      125.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 127.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      125.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 127.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      125.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 127.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      125.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 127.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      125.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 127.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      125.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 127.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      126.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 127.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      126.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 128.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      126.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 128.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      126.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 128.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      126.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 128.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      126.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 128.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      127.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 128.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      127.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 128.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      127.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 129.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      127.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 129.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      127.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 129.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      127.7s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 129.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      127.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 129.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      128.0s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 129.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      128.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 130.0s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      128.3s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 130.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      128.5s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 130.3s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      128.6s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 130.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      128.8s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 130.6s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      128.9s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 130.8s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      129.1s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 130.9s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      129.2s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 131.1s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      129.4s\n",
      "\u001b[2m => => # Successfully built optimum                                            \n",
      "\u001b[0m\u001b[2m => => # Installing collected packages: sentencepiece, pytz, mpmath, flatbuffer\n",
      "\u001b[0m\u001b[2m => => # s, xxhash, sympy, pyarrow, protobuf, multidict, humanfriendly, fsspec,\n",
      "\u001b[0m\u001b[2m => => #  frozenlist, dill, attrs, async-timeout, yarl, responses, pandas, onnx\n",
      "\u001b[0m\u001b[2m => => # runtime, onnx, multiprocess, coloredlogs, aiosignal, aiohttp, optimum,\n",
      "\u001b[0m\u001b[2m => => #  datasets                                                             \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 131.2s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      129.5s\n",
      "\u001b[2m => => # .2 attrs-21.4.0 coloredlogs-15.0.1 datasets-2.3.2 dill-0.3.5.1 flatbuf\n",
      "\u001b[0m\u001b[2m => => # fers-2.0 frozenlist-1.3.0 fsspec-2022.5.0 humanfriendly-10.0 mpmath-1.\n",
      "\u001b[0m\u001b[2m => => # 2.1 multidict-6.0.2 multiprocess-0.70.13 onnx-1.12.0 onnxruntime-1.11.\n",
      "\u001b[0m\u001b[2m => => # 1 optimum-1.2.3 pandas-1.4.2 protobuf-3.20.1 pyarrow-8.0.0 pytz-2022.1\n",
      "\u001b[0m\u001b[2m => => #  responses-0.18.0 sentencepiece-0.1.96 sympy-1.10.1 xxhash-3.0.0 yarl-\n",
      "\u001b[0m\u001b[2m => => # 1.7.2                                                                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 131.4s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      129.7s\n",
      "\u001b[2m => => # .2 attrs-21.4.0 coloredlogs-15.0.1 datasets-2.3.2 dill-0.3.5.1 flatbuf\n",
      "\u001b[0m\u001b[2m => => # fers-2.0 frozenlist-1.3.0 fsspec-2022.5.0 humanfriendly-10.0 mpmath-1.\n",
      "\u001b[0m\u001b[2m => => # 2.1 multidict-6.0.2 multiprocess-0.70.13 onnx-1.12.0 onnxruntime-1.11.\n",
      "\u001b[0m\u001b[2m => => # 1 optimum-1.2.3 pandas-1.4.2 protobuf-3.20.1 pyarrow-8.0.0 pytz-2022.1\n",
      "\u001b[0m\u001b[2m => => #  responses-0.18.0 sentencepiece-0.1.96 sympy-1.10.1 xxhash-3.0.0 yarl-\n",
      "\u001b[0m\u001b[2m => => # 1.7.2                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 131.5s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      129.8s\n",
      "\u001b[2m => => # .2 attrs-21.4.0 coloredlogs-15.0.1 datasets-2.3.2 dill-0.3.5.1 flatbuf\n",
      "\u001b[0m\u001b[2m => => # fers-2.0 frozenlist-1.3.0 fsspec-2022.5.0 humanfriendly-10.0 mpmath-1.\n",
      "\u001b[0m\u001b[2m => => # 2.1 multidict-6.0.2 multiprocess-0.70.13 onnx-1.12.0 onnxruntime-1.11.\n",
      "\u001b[0m\u001b[2m => => # 1 optimum-1.2.3 pandas-1.4.2 protobuf-3.20.1 pyarrow-8.0.0 pytz-2022.1\n",
      "\u001b[0m\u001b[2m => => #  responses-0.18.0 sentencepiece-0.1.96 sympy-1.10.1 xxhash-3.0.0 yarl-\n",
      "\u001b[0m\u001b[2m => => # 1.7.2                                                                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 131.7s (7/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[2m => => # .2 attrs-21.4.0 coloredlogs-15.0.1 datasets-2.3.2 dill-0.3.5.1 flatbuf\n",
      "\u001b[0m\u001b[2m => => # fers-2.0 frozenlist-1.3.0 fsspec-2022.5.0 humanfriendly-10.0 mpmath-1.\n",
      "\u001b[0m\u001b[2m => => # 2.1 multidict-6.0.2 multiprocess-0.70.13 onnx-1.12.0 onnxruntime-1.11.\n",
      "\u001b[0m\u001b[2m => => # 1 optimum-1.2.3 pandas-1.4.2 protobuf-3.20.1 pyarrow-8.0.0 pytz-2022.1\n",
      "\u001b[0m\u001b[2m => => #  responses-0.18.0 sentencepiece-0.1.96 sympy-1.10.1 xxhash-3.0.0 yarl-\n",
      "\u001b[0m\u001b[2m => => # 1.7.2                                                                 \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 131.8s (9/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m => [ 6/12] COPY ./optimum/ / /home/model-server/                          0.1s\n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "\u001b[4A\u001b[0G\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 132.0s (9/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m => [ 6/12] COPY ./optimum/ / /home/model-server/                          0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 132.1s (9/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m => [ 6/12] COPY ./optimum/ / /home/model-server/                          0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 132.3s (9/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m => [ 6/12] COPY ./optimum/ / /home/model-server/                          0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 132.4s (9/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m => [ 6/12] COPY ./optimum/ / /home/model-server/                          0.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 132.6s (9/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m => [ 6/12] COPY ./optimum/ / /home/model-server/                          0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 132.7s (9/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 132.9s (9/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 133.0s (9/16)                                                      \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 133.2s (10/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 133.3s (11/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 133.5s (11/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 133.6s (12/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 133.8s (13/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 133.9s (14/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 134.0s (14/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 134.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 134.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 134.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 134.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 134.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  0.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 134.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  0.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 135.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  1.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 135.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  1.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 135.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  1.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 135.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  1.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 135.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  1.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 135.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  1.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 136.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  1.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 136.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  2.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 136.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  2.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 136.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  2.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 136.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  2.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 136.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  2.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 136.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  2.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 137.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  3.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 137.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  3.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 137.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  3.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 137.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  3.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 137.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  3.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 137.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  3.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 138.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  3.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 138.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  4.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 138.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  4.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 138.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  4.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 138.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  4.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 138.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 138.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  4.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 139.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  5.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 139.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  5.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 139.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  5.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 139.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  5.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 139.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  5.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 139.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  5.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 139.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  5.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 140.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  6.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 140.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  6.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 140.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  6.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 140.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  6.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 140.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  6.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 140.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  6.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 141.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  6.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 141.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  7.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 141.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  7.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 141.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  7.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 141.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  7.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 141.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  7.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 141.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  7.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 142.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  8.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 142.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  8.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 142.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  8.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 142.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  8.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 142.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  8.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 142.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  8.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 142.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  8.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 143.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  9.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 143.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  9.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 143.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  9.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 143.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  9.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 143.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  9.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 143.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  9.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 144.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedde  9.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 144.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  10.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 144.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  10.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 144.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  10.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 144.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  10.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 144.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  10.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 144.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  10.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 145.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  11.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 145.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  11.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 145.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  11.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 145.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  11.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 145.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  11.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 145.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  11.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 146.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  11.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 146.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  12.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 146.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  12.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 146.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  12.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 146.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  12.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 146.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  12.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 146.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  12.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 147.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  13.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 147.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  13.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 147.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  13.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 147.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  13.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 147.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  13.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 147.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  13.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 147.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  13.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 148.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  14.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 148.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  14.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 148.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  14.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 148.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  14.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 148.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  14.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 148.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  14.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 149.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  14.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 149.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  15.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 149.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  15.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 149.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  15.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 149.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  15.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 149.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  15.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 149.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  15.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 150.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  16.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 150.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  16.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 150.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  16.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 150.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  16.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 150.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  16.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 150.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  16.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 150.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  16.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 151.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  17.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 151.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  17.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 151.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  17.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 151.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  17.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 151.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  17.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 151.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  17.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 152.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  17.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 152.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  18.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 152.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  18.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 152.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  18.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 152.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  18.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 152.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  18.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 152.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  18.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 153.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  19.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 153.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  19.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 153.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  19.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 153.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  19.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 153.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  19.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 153.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  19.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 154.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  19.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 154.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  20.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 154.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  20.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 154.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  20.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 154.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  20.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 154.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  20.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 154.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  20.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 155.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  21.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 155.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  21.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 155.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  21.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 155.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  21.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 155.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  21.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 155.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  21.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 155.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  21.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 156.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  22.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 156.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  22.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 156.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  22.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 156.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  22.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 156.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  22.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 156.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  22.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 157.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  22.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 157.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  23.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 157.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  23.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 157.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  23.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 157.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  23.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 157.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  23.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 157.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  23.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 158.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  24.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 158.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  24.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 158.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  24.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 158.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  24.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 158.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  24.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 158.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  24.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 158.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  24.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 159.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  25.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 159.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  25.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 159.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  25.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 159.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  25.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 159.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  25.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 159.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  25.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 160.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  25.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 160.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  26.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 160.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  26.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 160.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  26.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 160.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  26.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 160.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  26.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 160.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  26.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 161.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  27.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 161.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  27.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 161.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  27.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 161.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  27.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 161.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  27.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 161.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  27.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 161.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  27.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 162.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  28.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 162.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  28.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 162.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  28.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 162.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  28.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 162.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  28.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 162.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  28.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 163.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  28.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 163.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  29.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 163.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  29.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 163.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  29.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 163.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  29.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 163.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  29.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 163.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  29.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 164.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  30.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 164.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  30.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 164.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  30.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 164.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  30.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 164.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  30.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 164.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  30.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 165.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  30.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 165.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  31.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 165.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  31.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 165.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  31.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 165.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  31.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 165.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  31.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 165.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  31.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 166.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  32.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 166.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  32.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 166.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  32.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 166.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  32.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 166.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  32.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 166.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  32.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 166.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  32.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 167.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  33.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 167.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  33.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 167.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  33.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 167.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  33.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 167.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  33.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 167.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  33.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 168.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  33.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 168.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  34.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 168.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  34.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 168.4s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  34.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 168.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  34.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 168.7s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  34.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 168.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  34.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 169.0s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  35.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 169.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  35.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 169.3s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  35.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 169.5s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  35.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 169.6s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  35.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 169.8s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  35.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 169.9s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  35.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 170.1s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 170.2s (15/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 170.4s (16/16)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 170.5s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     0.1s\n",
      " => => exporting layers                                                    0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 170.7s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     0.3s\n",
      " => => exporting layers                                                    0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 170.8s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     0.5s\n",
      " => => exporting layers                                                    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 171.0s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     0.6s\n",
      " => => exporting layers                                                    0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 171.1s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     0.8s\n",
      " => => exporting layers                                                    0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 171.3s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     0.9s\n",
      " => => exporting layers                                                    0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 171.4s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     1.1s\n",
      " => => exporting layers                                                    1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 171.6s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     1.2s\n",
      " => => exporting layers                                                    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 171.7s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     1.4s\n",
      " => => exporting layers                                                    1.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 171.9s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     1.5s\n",
      " => => exporting layers                                                    1.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 172.0s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     1.7s\n",
      " => => exporting layers                                                    1.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 172.2s (16/17)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m => exporting to image                                                     1.8s\n",
      " => => exporting layers                                                    1.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 172.3s (17/17) FINISHED                                            \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.74kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   1.6s\n",
      "\u001b[0m\u001b[34m => [ 1/12] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:86fd15414  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          7.4s\n",
      "\u001b[0m\u001b[34m => => transferring context: 266.49MB                                      7.4s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/12] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/12] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => [ 4/12] RUN pip3 install 'optimum[onnxruntime]'                      130.0s\n",
      "\u001b[0m\u001b[34m => [ 5/12] COPY custom_handler.py /home/model-server/                     0.0s\n",
      "\u001b[0m\u001b[34m => [ 6/12] COPY ./optimum/ / /home/model-server/                          1.3s\n",
      "\u001b[0m\u001b[34m => [ 7/12] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.2s\n",
      "\u001b[0m\u001b[34m => [ 8/12] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.2s\n",
      "\u001b[0m\u001b[34m => [ 9/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [10/12] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [11/12] RUN printf \"\\nworkers=4\" >> /home/model-server/config.propert  0.2s\n",
      "\u001b[0m\u001b[34m => [12/12] RUN torch-model-archiver -f   --model-name=test_sbert_embedd  36.3s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     1.9s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    1.9s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:9c77187e4f3dd23c266099c9df20d2ddcdb6f0a1d2bae  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to gcr.io/huggingface-ml/pytorch_predict_test_sbert_embedde  0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\r\n",
      "Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\r\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "  --tag=$CUSTOM_PREDICTOR_IMAGE_URI \\\n",
    "  ./predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bdd6f",
   "metadata": {},
   "source": [
    "### Run container locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d23d9ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_sbert_embedder_optimum\n",
      "WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      "380ad2454bb71d0e6c40da1302bd057724bb4b7160f008de4f51a8d89d958682\n"
     ]
    }
   ],
   "source": [
    "!docker stop local_sbert_embedder_optimum\n",
    "!docker run -t -d --rm -p 7080:7080 --name=local_sbert_embedder_optimum $CUSTOM_PREDICTOR_IMAGE_URI\n",
    "!sleep 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df6271",
   "metadata": {},
   "source": [
    "### Test API locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f66cc11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"status\": \"Healthy\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2dcbddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base64: unrecognized option `--wrap=0'\n",
      "Usage:\tbase64 [-hvDd] [-b num] [-i in_file] [-o out_file]\n",
      "  -h, --help     display this message\n",
      "  -Dd, --decode   decodes input\n",
      "  -b, --break    break encoded string into num character lines\n",
      "  -i, --input    input file (default: \"-\" for stdin)\n",
      "  -o, --output   output file (default: \"-\" for stdout)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [[0.15652182698249817, -0.007943082600831985, -0.02663888782262802, -0.08307340741157532, 0.10856898128986359, 0.1659667044878006, 0.509436845779419, 0.18165743350982666, -0.1608804315328598, -0.32550927996635437, -0.3147802948951721, 0.11809796839952469, -0.2053718864917755, 0.16539841890335083, 0.0431254580616951, 0.14275845885276794, 0.33132243156433105, 0.05910756438970566, -0.07154326885938644, -0.34179458022117615, -0.3615402281284332, -0.18151073157787323, -0.015651991590857506, -0.2512113153934479, -0.17122207581996918, -0.2773536145687103, -0.3305656909942627, -0.5285634994506836, -0.038842517882585526, 0.0860275998711586, -0.12963564693927765, -0.016775203868746758, 0.10492599010467529, 0.06866931915283203, -0.058284010738134384, 0.131710484623909, 0.013576209545135498, -0.14154024422168732, -0.210587739944458, -0.05883493274450302, 0.15064772963523865, -0.32615822553634644, 0.11842171102762222, 0.17478305101394653, -0.2231474071741104, 0.02912208065390587, -1.582545518875122, 0.49616360664367676, -0.058493636548519135, 0.16077661514282227, 0.19926494359970093, 0.06111591309309006, 0.24502034485340118, 0.2472367137670517, 0.22260385751724243, -0.25913238525390625, -0.04897475987672806, -0.4203648567199707, -0.2146930694580078, -0.3297344446182251, 0.42602667212486267, 0.4174309968948364, 0.054189182817935944, -0.12818750739097595, 0.0900702178478241, 0.32074615359306335, 0.04517148807644844, -0.07783860713243484, -0.7556832432746887, 0.035848937928676605, 0.1393531858921051, 0.32375702261924744, -0.15387147665023804, -0.16454866528511047, -0.0030393265187740326, 0.16405251622200012, -0.39852821826934814, 0.21632638573646545, 0.16089501976966858, -0.05020167678594589, -0.3413812518119812, 0.39225491881370544, 0.31386733055114746, 0.16466087102890015, -0.1586310714483261, -0.04557356610894203, 0.271302729845047, 0.07342030107975006, -0.2554163336753845, 0.3801449239253998, -0.21272170543670654, -0.1057591661810875, 0.1173868328332901, -0.09341564774513245, 0.22665072977542877, 0.014028050005435944, 0.2318933755159378, -0.3593151867389679, -0.1678057760000229, 0.045781493186950684, -0.4566827118396759, -0.10362947732210159, 0.18796004354953766, 0.25864818692207336, -0.20987248420715332, 0.04318207502365112, -0.07038208097219467, -0.3073684275150299, -0.07726474106311798, 0.058501675724983215, 0.4687668979167938, -0.5312599539756775, 0.10756272822618484, -0.4711751341819763, 0.0008883848786354065, 0.062290724366903305, 0.025154665112495422, -0.34145310521125793, 0.1406891942024231, -0.02080925926566124, -0.08533333986997604, 0.12701578438282013, 0.1976601630449295, -0.15402507781982422, -0.17524199187755585, -0.4450138509273529, -0.08131510019302368, -0.16222138702869415, -0.5294429063796997, -0.19138474762439728, 0.513059139251709, -0.383533239364624, -0.19954948127269745, 0.2120564579963684, -0.10510402917861938, 0.4527510702610016, 0.05669922009110451, -0.18983817100524902, 0.043303705751895905, 0.006867183838039637, -0.21362510323524475, 0.08255063742399216, -0.6566327214241028, 0.21147575974464417, 0.3692263662815094, -0.5380309820175171, 0.2871161997318268, 0.10077613592147827, -0.1913568079471588, -0.1851484477519989, 0.14877860248088837, 0.5248452425003052, -0.45964139699935913, -0.3198785185813904, -0.35399195551872253, -0.2381161004304886, 0.03979329764842987, 0.07037470489740372, 0.16946527361869812, 0.05950449779629707, 0.023987721651792526, 0.17609280347824097, -0.03553393483161926, -0.09468007832765579, 0.44173064827919006, 0.28347429633140564, 0.2706027030944824, 0.058880146592855453, 0.12036833167076111, -0.3000660836696625, -0.04575200378894806, -0.14897757768630981, 0.19581758975982666, 0.1263091117143631, -0.5964033603668213, 0.23734687268733978, 0.13535790145397186, 0.421019583940506, 0.01697300374507904, -0.04159103333950043, -0.7966213226318359, -0.04008973389863968, 0.01868274062871933, 0.22212064266204834, -0.497897744178772, -0.15088701248168945, -0.025851868093013763, 0.0063356803730130196, 0.0317736491560936, 0.04648306593298912, -0.21970945596694946, -0.3668009042739868, -0.09195124357938766, -0.1965305656194687, 0.2517951428890228, -0.47216227650642395, -0.4637482762336731, 0.33184248208999634, 0.19169609248638153, 0.19915001094341278, 0.0766698345541954, -0.2835262715816498, 0.15357856452465057, -0.05112519860267639, -0.1622181236743927, 0.5681605935096741, 0.16413947939872742, 0.054035309702157974, -0.25692543387413025, 0.12957310676574707, -0.28720465302467346, 0.13811913132667542, 0.022735975682735443, 0.009000906720757484, 0.0833226889371872, 0.11270618438720703, -0.2636060118675232, -0.09058113396167755, -0.1660660058259964, -0.17853213846683502, 0.31996259093284607, 0.0005053989589214325, -0.3535781800746918, -0.10085329413414001, -0.031897902488708496, 0.6008026599884033, 0.5094015598297119, -0.045480772852897644, 0.0147872194647789, -0.685092568397522, 0.20794594287872314, -0.23549553751945496, 0.0066954270005226135, 0.22211743891239166, -0.5433058738708496, 0.3843064308166504, -0.11062473803758621, 0.08647339046001434, 0.08239781856536865, -0.4605475962162018, -0.18186908960342407, -0.07841141521930695, 0.18047255277633667, 0.4330919682979584, 0.08954808115959167, 0.4464420974254608, 0.1676989644765854, 0.1388135850429535, 0.30150625109672546, -0.4250432550907135, -0.411337673664093, 0.8697729110717773, 0.3394140899181366, 0.19913221895694733, -0.19391033053398132, 0.19954991340637207, 0.05340924486517906, 0.16968068480491638, 0.0396491140127182, 0.09078799188137054, -0.11671237647533417, -0.19672814011573792, -0.16403552889823914, -0.14612743258476257, -0.4672077000141144, -0.2503192126750946, 0.09392920136451721, 0.031035957857966423, -0.006058810278773308, -0.4210793077945709, -0.1843339055776596, 0.26664814352989197, -0.27064207196235657, -0.09741652756929398, -0.052206721156835556, -0.5519019365310669, 0.4135587215423584, 0.7389915585517883, -0.02119304984807968, -0.5638700127601624, -0.2770738899707794, 0.15328198671340942, -0.044816866517066956, -0.0011450350284576416, 0.015172399580478668, 0.06479943543672562, -0.30937182903289795, -0.4178997874259949, 0.15342730283737183, 0.0873388946056366, 0.21749617159366608, 0.09430863708257675, 0.1530296504497528, -0.25874900817871094, 0.516639769077301, -0.01968291588127613, 0.037526167929172516, 0.14869537949562073, -0.06625984609127045, 0.09888698905706406, -0.4816877841949463, 0.2128996104001999, 0.5473743677139282, -0.046602584421634674, 0.17529195547103882, 0.0541880801320076, -0.16882196068763733, -0.05740412324666977, -8.219982147216797, -0.08503641188144684, -0.12049885839223862, -0.45517459511756897, 0.18238580226898193, -0.05003911629319191, -0.33677083253860474, -0.02077874168753624, -0.22663964331150055, -0.016765939071774483, -0.030421003699302673, -0.3010822534561157, -0.01939363405108452, 0.10656441748142242, 0.2743285000324249, -0.047460369765758514, -0.3116806149482727, 0.027206983417272568, -0.6071531176567078, -0.1304176300764084, 0.13335862755775452, -0.23237787187099457, 0.1455497443675995, 0.07473762333393097, 0.16305148601531982, -0.3974723219871521, -0.41499564051628113, 0.052235521376132965, 0.09629757702350616, -0.03919236734509468, -0.1510421335697174, -0.4783145487308502, -0.22724978625774384, -0.036251675337553024, 0.05347482115030289, -0.2792564034461975, 0.24804812669754028, -0.16805578768253326, 0.36820700764656067, 0.04417578876018524, -0.199130579829216, 0.25285834074020386, 0.13461869955062866, -0.023357072845101357, 0.03627374395728111, 0.15362334251403809, 0.03254017233848572, -0.16071194410324097, -0.2942277491092682, 0.44915711879730225, 0.6711748838424683, 0.16025757789611816, 0.3903947174549103, -0.5159028172492981, -0.0032502412796020508, 0.14245280623435974, 0.2543621063232422, -0.046867214143276215, -0.22869735956192017, 0.1694602221250534, 0.1199406087398529, 0.23886114358901978, -0.012627098709344864, 0.04204259067773819, 0.429364949464798, -0.026887310668826103, -0.3961139917373657, 0.0010364744812250137, 0.42753949761390686, 0.3167859613895416, 0.14504985511302948, -0.18348315358161926, 0.019150592386722565, -1.7879912853240967, -0.08380072563886642, -0.3394691050052643, 0.20974457263946533, 0.010220468044281006, -0.1826644241809845, -0.06859836727380753, -0.05423640459775925, 0.06356070935726166, 0.10582181066274643, 0.21654659509658813, 0.23545576632022858, -0.17753691971302032, -0.5421593189239502, 0.20098137855529785, 0.14512309432029724, 0.27784475684165955, 0.4816712737083435, -0.1305721551179886, -0.13211633265018463, -0.03191719204187393, 0.21495039761066437, -0.05044475197792053, -0.12959101796150208, 0.024572160094976425, 0.11621478945016861, 0.170610249042511, 0.25835883617401123, 0.13573859632015228, -0.35570600628852844, 0.08527328819036484, 0.06684570014476776, -0.2275402843952179, -0.3295769691467285, 0.19228577613830566, 0.30912846326828003, -0.27105632424354553, -0.2892136871814728, 0.21537809073925018, 0.18169856071472168, -0.1645754873752594, 0.5698809027671814, 0.039945200085639954, -0.07613620907068253, 0.31930026412010193, 0.06418661773204803, -0.1265166699886322, 0.09811370074748993, 0.047075316309928894, -0.009033456444740295, -0.08408313989639282, -0.1761813461780548, 0.1136961579322815, -0.06439820677042007, 0.025678317993879318, 0.28867340087890625, -0.18731941282749176, 0.10419231653213501, 0.33962303400039673, 0.4127058982849121, -0.23932385444641113, 0.2344755232334137, 0.07918999344110489, -0.1778322458267212, -0.04025690630078316, -0.3591136932373047, 0.46320652961730957, 0.29688894748687744, -0.0034757032990455627, -0.10428240895271301, -0.07194546610116959, -0.12182597070932388, 0.09412604570388794, 0.018868163228034973, 0.0065848976373672485, 0.10239584743976593, 0.10957812517881393, 0.06237272918224335, 0.45647096633911133, 0.38227635622024536, -0.14636215567588806, 0.04465937614440918, -0.06169857084751129, -0.10916534811258316, -0.16015681624412537, 0.010045701637864113, -0.2754369378089905, 0.0036322027444839478, -0.062016189098358154, -1.1510937213897705, -0.2478131502866745, -0.07104828208684921, 0.29629671573638916, -0.020404502749443054, 0.09335234761238098, 0.12738560140132904, -0.1622718870639801, 0.12194932252168655, -0.5012385249137878, 0.021009866148233414, 0.46782219409942627, -0.1866808533668518, 0.24850289523601532, -0.04897012934088707, -0.3367273211479187, -0.06748643517494202, 0.17998787760734558, 0.10524599254131317, 0.21692904829978943, -0.1178014874458313, -0.2643323242664337, 0.32779791951179504, 0.12435039132833481, 0.3058192729949951, -0.013119509443640709, 0.2637396454811096, -0.047101475298404694, 0.12431532144546509, 0.28346842527389526, 0.07938447594642639, 0.1259322464466095, -0.004240959882736206, -0.16577187180519104, 0.20692911744117737, 0.7263293862342834, -0.049291618168354034, 0.45510613918304443, -0.01844589039683342, -0.44625124335289, -0.08577227592468262, 0.18826189637184143, 0.05918871611356735, 0.35204145312309265, 0.022658802568912506, -0.44721266627311707, 0.40099969506263733, 0.07269586622714996, 0.4345490336418152, -0.013571023941040039, -0.27924907207489014, 0.26790064573287964, 0.6641532182693481, -0.25607889890670776, 0.2608504891395569, -0.3047485649585724, -0.4712222218513489, -0.2041730135679245, -0.2184896469116211, -0.08519897609949112, 0.05333707481622696, -0.37936830520629883, -0.135226309299469, -0.09875699877738953, 0.08095815777778625, -0.11648866534233093, -0.06390269845724106, -0.0451483316719532, -0.15444017946720123, 0.39294078946113586, -0.18057744204998016, 0.11265276372432709, 0.07019039988517761, 0.23102359473705292, -0.21140816807746887, 0.4042980968952179, -0.37542441487312317, 0.2884403467178345, 0.27971118688583374, -0.12472258508205414, -0.32178089022636414, -0.20592230558395386, 0.15103304386138916, 0.03978767246007919, -0.1423605978488922, -0.11884415149688721, -0.006144575774669647, -0.03773781657218933, -0.21055787801742554, -0.08676396310329437, -0.1691134124994278, 0.3647457957267761, -0.11109647154808044, -0.31010061502456665, -0.32396286725997925, -0.13487622141838074, -0.0721491351723671, -0.1295880377292633, -0.1161627322435379, 0.23672695457935333, -0.19650354981422424, -0.12322704493999481, 0.10732852667570114, 0.2460540235042572, 0.047724347561597824, 0.17877843976020813, 0.17174138128757477, -0.031995028257369995, 0.1529504358768463, 0.01535060815513134, 0.31747967004776, 0.36679962277412415, -0.018014216795563698, 0.25110694766044617, -0.43479031324386597, 0.03089667111635208, -0.15081346035003662, 0.07924120873212814, -0.03434969112277031, 0.18613433837890625, -0.22536376118659973, -0.2338586449623108, 0.13540242612361908, -0.21783533692359924, 0.40783342719078064, 0.22913578152656555, -0.24688854813575745, 0.05925116688013077, -0.056128811091184616, 0.17359939217567444, -0.2601425349712372, -0.055288828909397125, 0.21207430958747864, -0.12738849222660065, 0.228202685713768, 0.4570332467556, -0.4121922552585602, 0.33955544233322144, -0.17928092181682587, 0.3164791762828827, 0.3418010175228119, -0.6847292184829712, -0.10313788801431656, 0.18114086985588074, -0.06524089723825455, -0.13504378497600555, -0.10070779919624329, 0.1650855839252472, 0.10132592171430588, 0.05280314385890961, 0.2884734570980072, 0.1825920045375824, -0.4632610082626343, 0.1727810949087143, 0.3381083607673645, -0.15787871181964874, 0.13061554729938507, 0.2058107852935791, -0.09887713938951492, 0.0870780274271965, -0.18197187781333923, 0.2003200650215149, 0.3982354998588562, 0.6350252032279968, -0.13083115220069885, 0.06512357294559479, 0.17663633823394775, 0.38092368841171265, 0.17945528030395508, -0.052986785769462585, 0.006643995642662048, 0.2947288155555725, 0.20991799235343933, -0.251709908246994, 0.2396039515733719, 0.28995269536972046, 0.11678767204284668, 0.4954156279563904, 0.3874832093715668, 0.40919485688209534, -0.6758400797843933, 0.24526503682136536, -0.07416821271181107, 0.22660908102989197, 0.17763511836528778, -0.07799973338842392, -0.22671261429786682, 0.45365241169929504, -0.08708441257476807, 0.03451904281973839, -0.16803425550460815, 0.22158092260360718, -0.2283150553703308, -0.1988964080810547, 0.4203691780567169, 0.38688772916793823, -0.1628112643957138, 0.09703130275011063, 0.2709079384803772, 0.17040149867534637, 0.30043330788612366, 0.2703792154788971, -0.10574778914451599, 0.0653865784406662, -0.15075081586837769, 0.028366751968860626, -0.35602086782455444, -0.15102790296077728, -0.053676243871450424, -0.16718772053718567, -0.1745726764202118, 0.3083580434322357, 0.004441753029823303, -0.07091625779867172, 0.1867990642786026, 0.1337043046951294, -0.0522911474108696, -0.1538042426109314, -0.5059973001480103, -0.05216485261917114, 0.07191222161054611, -0.24099764227867126, 0.13100740313529968, 0.13920184969902039, 0.17727287113666534, 0.06367195397615433, -0.10754217207431793, -0.21722424030303955, 0.4463425874710083, -0.2942112982273102, -0.19251081347465515, 0.3700822591781616, 0.08507341146469116, -0.1706976741552353, -0.02692801132798195, -0.029734600335359573, 0.29267895221710205, -0.11304621398448944, 0.08414657413959503, -0.3572084307670593, -0.14874452352523804, -0.06878829002380371, -0.09065768122673035, 0.6503586173057556, 0.16117793321609497, -0.33897778391838074, -0.13496249914169312, 0.08298459649085999, 0.49609723687171936, -0.02658720687031746, -0.2973184883594513, 0.4031775891780853, -0.2684271037578583, 0.25570371747016907, 0.12475095689296722, 0.4956583082675934, 0.2616512179374695, 0.013773411512374878, 0.1640939861536026, 0.39328694343566895, -0.08149124681949615, 0.24163918197155, -0.5773971080780029, -0.19386497139930725, 0.14484255015850067, -0.21944113075733185, -0.47668033838272095, 0.31497296690940857, 0.1283572018146515, 0.1520024985074997, -0.32843196392059326, 0.2567504644393921, -0.26194095611572266, -0.05268753319978714, -0.11702325940132141, -0.3194367289543152, -0.14500591158866882, -0.6907806396484375, 0.22342129051685333, 0.16711010038852692, -0.359439879655838, -0.12767773866653442, -0.5912354588508606, -0.022987976670265198, 0.060915086418390274, 0.2745642066001892, 0.27724385261535645, 0.5106984376907349, 0.09495428204536438, 0.3565896153450012, -0.09593832492828369, -0.4433579742908478, -0.020850127562880516, -0.34209370613098145, 0.261099636554718, 0.10201888531446457, -0.14967922866344452, -0.32788512110710144]]}"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat > ./predictor/instances.json <<END\n",
    "{ \n",
    "   \"instances\": [\n",
    "     { \n",
    "       \"data\": {\n",
    "         \"b64\": \"$(echo 'I am so happy to be at Deauville today' | base64 --wrap=0)\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./predictor/instances.json \\\n",
    "  http://localhost:7080/predictions/$APP_NAME/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b40e9e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/huggingface-ml/pytorch_predict_test_sbert_embedder_optimum'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c5d0c",
   "metadata": {},
   "source": [
    "### Push to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2611ed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/huggingface-ml/pytorch_predict_test_sbert_embedder]\n",
      "\n",
      "\u001b[1Bc22e8bb9: Preparing \n",
      "\u001b[1Bb5b7f6ab: Preparing \n",
      "\u001b[1B4d59d004: Preparing \n",
      "\u001b[1Badc9f621: Preparing \n",
      "\u001b[1B0763950f: Preparing \n",
      "\u001b[1Bd9e635c4: Preparing \n",
      "\u001b[1B633ebd40: Preparing \n",
      "\u001b[1Bff3792de: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1Bda41ec4a: Preparing \n",
      "\u001b[1Bd007c81a: Preparing \n",
      "\u001b[1Bc83319d2: Preparing \n",
      "\u001b[1B005ec070: Preparing \n",
      "\u001b[1Bc8ae3daf: Preparing \n",
      "\u001b[1Be9bfffc1: Preparing \n",
      "\u001b[1B51f4d794: Preparing \n",
      "\u001b[12B9e635c4: Waiting g unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication\n"
     ]
    }
   ],
   "source": [
    "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0cb4c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://florent-bucket\"  # <---CHANGE THIS TO YOUR BUCKET\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "615eab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil mb -l $REGION $BUCKET_NAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9565b",
   "metadata": {},
   "source": [
    "### Create model and endpoint to VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d5f00242",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "170d5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
    "model_description = \"PyTorch based sentence transformers embedder with custom container\"\n",
    "\n",
    "MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
    "serving_container_ports = [7080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6eab0c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/1049843053967/locations/us-central1/models/5777542177823391744/operations/622373677819756544\n",
      "Model created. Resource name: projects/1049843053967/locations/us-central1/models/5777542177823391744\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1049843053967/locations/us-central1/models/5777542177823391744')\n",
      "test_sbert_embedder_optimum-v1\n",
      "projects/1049843053967/locations/us-central1/models/5777542177823391744\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e3863a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/1049843053967/locations/us-central1/endpoints/8736868927889473536/operations/1707741188016046080\n",
      "Endpoint created. Resource name: projects/1049843053967/locations/us-central1/endpoints/8736868927889473536\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/1049843053967/locations/us-central1/endpoints/8736868927889473536')\n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea432646",
   "metadata": {},
   "source": [
    "### Deploy endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ecad0cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/1049843053967/locations/us-central1/endpoints/8736868927889473536\n",
      "Deploy Endpoint model backing LRO: projects/1049843053967/locations/us-central1/endpoints/8736868927889473536/operations/3306519055732572160\n",
      "Endpoint model deployed. Resource name: projects/1049843053967/locations/us-central1/endpoints/8736868927889473536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x156698ca0> \n",
       "resource name: projects/1049843053967/locations/us-central1/endpoints/8736868927889473536"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-8\"\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3a405988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint display name = test_sbert_embedder_optimum-endpoint resource id =projects/1049843053967/locations/us-central1/endpoints/8736868927889473536 \n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "filter = f'display_name=\"{endpoint_display_name}\"'\n",
    "\n",
    "for endpoint_info in aiplatform.Endpoint.list(filter=filter):\n",
    "    print(\n",
    "        f\"Endpoint display name = {endpoint_info.display_name} resource id ={endpoint_info.resource_name} \"\n",
    "    )\n",
    "\n",
    "endpoint = aiplatform.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c937f2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: \"5204054504961474560\"\n",
       " model: \"projects/1049843053967/locations/us-central1/models/5777542177823391744\"\n",
       " display_name: \"test_sbert_embedder_optimum-v1\"\n",
       " create_time {\n",
       "   seconds: 1655828146\n",
       "   nanos: 861195000\n",
       " }\n",
       " dedicated_resources {\n",
       "   machine_spec {\n",
       "     machine_type: \"n1-standard-8\"\n",
       "   }\n",
       "   min_replica_count: 1\n",
       "   max_replica_count: 3\n",
       " }]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "16cf7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [\n",
    "    b\"This is an example of model deployment using a sentence transformers model and optimum\",\n",
    "]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "960aa21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a19a344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tThis is an example of model deployment using a sentence transformers model and optimum\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGhpcyBpcyBhbiBleGFtcGxlIG9mIG1vZGVsIGRlcGxveW1lbnQgdXNpbmcgYSBzZW50ZW5jZSB0cmFuc2Zvcm1lcnMgbW9kZWwgYW5kIG9wdGltdW0=\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "CPU times: user 22.2 s, sys: 4.62 s, total: 26.8 s\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"=\" * 1000)\n",
    "for instance in test_instances:\n",
    "    print(f\"Input text: \\n\\t{instance.decode('utf-8')}\\n\")\n",
    "    b64_encoded = base64.b64encode(instance)\n",
    "    test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
    "    print(f\"Formatted input: \\n{json.dumps(test_instance, indent=4)}\\n\")\n",
    "    prediction = endpoint.predict(instances=test_instance)\n",
    "    #print(f\"Prediction response: \\n\\t{prediction}\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "db1a6d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 ms, sys: 4.16 ms, total: 21.8 ms\n",
      "Wall time: 208 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction = endpoint.predict(instances=test_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0f260939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(\"This is an example of model deployment using a sentence transformers model and optimum\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "587683e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction.predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3e143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('venv_hf_3.9.10')",
   "language": "python",
   "name": "python3910jvsc74a57bd041d616a00e55b30810a056d2ec88dde9c7f0c29a440bd149fc2105d405ad956d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
